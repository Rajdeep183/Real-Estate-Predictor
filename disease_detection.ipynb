{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajdeep183/Real-Estate-Predictor/blob/main/disease_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nv8YHcAnZes",
        "outputId": "7ddd90bc-d4ee-467e-ad43-782919cd2b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PlantVillage-Dataset'...\n",
            "remote: Enumerating objects: 163235, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 163235 (delta 2), reused 1 (delta 0), pack-reused 163229 (from 1)\u001b[K\n",
            "Receiving objects: 100% (163235/163235), 2.00 GiB | 28.17 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n",
            "Updating files: 100% (182401/182401), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/spMohanty/PlantVillage-Dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define paths\n",
        "base_dir = \"./PlantVillage-Dataset/raw/color\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "valid_dir = os.path.join(base_dir, \"valid\")\n",
        "\n",
        "# Create train and valid directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(valid_dir, exist_ok=True)\n",
        "\n",
        "# Get all class (disease) folders, excluding 'train' and 'valid'\n",
        "disease_classes = [d for d in os.listdir(base_dir)\n",
        "                   if os.path.isdir(os.path.join(base_dir, d)) and d not in ['train', 'valid']]\n",
        "\n",
        "# Split images into train (80%) and valid (20%)\n",
        "for disease in disease_classes:\n",
        "    disease_path = os.path.join(base_dir, disease)\n",
        "    images = os.listdir(disease_path)\n",
        "\n",
        "    # Skip if there are no images in the folder\n",
        "    if len(images) == 0:\n",
        "        continue\n",
        "\n",
        "    # Split the dataset\n",
        "    train_images, valid_images = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create new class folders inside train and valid directories\n",
        "    os.makedirs(os.path.join(train_dir, disease), exist_ok=True)\n",
        "    os.makedirs(os.path.join(valid_dir, disease), exist_ok=True)\n",
        "\n",
        "    # Move training images to the class subfolder within train_dir\n",
        "    for img in train_images:\n",
        "        src = os.path.join(disease_path, img)\n",
        "        dst = os.path.join(train_dir, disease, img)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "    # Move validation images to the class subfolder within valid_dir\n",
        "    for img in valid_images:\n",
        "        src = os.path.join(disease_path, img)\n",
        "        dst = os.path.join(valid_dir, disease, img)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "print(\"Dataset successfully split into train and valid folders!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoXPpUrUn-tH",
        "outputId": "7c654e40-999f-4e08-c702-31618b78e0c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully split into train and valid folders!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations (resize + convert to tensor)\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
        "\n",
        "# Load train and validation datasets\n",
        "train_dir = \"./PlantVillage-Dataset/raw/color/train\"\n",
        "valid_dir = \"./PlantVillage-Dataset/raw/color/valid\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "valid_dataset = datasets.ImageFolder(root=valid_dir, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Valid size: {len(valid_dataset)}\")"
      ],
      "metadata": {
        "id": "78Af0OAzoEuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab6387e-a873-4917-ce98-5c11121b2ffa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 43429, Valid size: 10876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224 (for pretrained models)\n",
        "    transforms.RandomHorizontalFlip(),  # Data Augmentation\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize for pretrained models\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dir = \"/content/PlantVillage-Dataset/raw/color/train\"\n",
        "valid_dir = \"/content/PlantVillage-Dataset/raw/color/valid\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "valid_dataset = datasets.ImageFolder(root=valid_dir, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Get class names\n",
        "class_names = train_dataset.classes\n",
        "print(f\"Classes: {class_names}, Total: {len(class_names)}\")\n"
      ],
      "metadata": {
        "id": "FIXu6Z_FoRF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751e67e0-97b4-455e-b3f5-bdb250a7ef80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy'], Total: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Use EfficientNet-B3 for better performance\n",
        "model = models.efficientnet_b3(pretrained=True)\n",
        "\n",
        "# Freeze early layers for transfer learning\n",
        "for param in model.features[:-3].parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Custom classifier with dropout for regularization\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(512, len(class_names))\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "print(f\"EfficientNet-B3 model ready with {len(class_names)} classes!\")\n"
      ],
      "metadata": {
        "id": "xIqFejG5oWwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a617768-dfe8-4017-b467-4b2edfc4ef3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientNet-B3 model ready with 38 classes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing for better generalization\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # AdamW with weight decay\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=3\n",
        ")\n"
      ],
      "metadata": {
        "id": "guYTyi89ociR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace training loop with validation tracking, early stopping, and progress bars\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "num_epochs = 5\n",
        "best_val_acc = 0.0\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EPOCH {epoch+1}/{num_epochs}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    # Training progress bar\n",
        "    train_pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\",\n",
        "                     ncols=100, colour='blue', leave=False)\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_pbar):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # Update progress bar with current metrics\n",
        "        current_acc = 100 * correct / total\n",
        "        current_loss = running_loss / (batch_idx + 1)\n",
        "        train_pbar.set_postfix({\n",
        "            'Loss': f'{current_loss:.4f}',\n",
        "            'Acc': f'{current_acc:.2f}%'\n",
        "        })\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # Validation progress bar\n",
        "    val_pbar = tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}\",\n",
        "                   ncols=100, colour='green', leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(val_pbar):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "            # Update progress bar with current metrics\n",
        "            current_val_acc = 100 * val_correct / val_total\n",
        "            current_val_loss = val_loss / (batch_idx + 1)\n",
        "            val_pbar.set_postfix({\n",
        "                'Loss': f'{current_val_loss:.4f}',\n",
        "                'Acc': f'{current_val_acc:.2f}%'\n",
        "            })\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss_avg = val_loss / len(valid_loader)\n",
        "\n",
        "    # Epoch summary\n",
        "    print(f\"\\nüìä EPOCH {epoch+1} SUMMARY:\")\n",
        "    print(f\"   üîµ Train - Loss: {avg_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"   üü¢ Valid - Loss: {val_loss_avg:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(val_acc)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    if new_lr != old_lr:\n",
        "        print(f\"   üìâ Learning rate reduced: {old_lr:.6f} ‚Üí {new_lr:.6f}\")\n",
        "\n",
        "    # Early stopping and model saving\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_plant_disease_model.pth\")\n",
        "        patience_counter = 0\n",
        "        print(f\"   ‚≠ê NEW BEST MODEL! Validation Accuracy: {val_acc:.2f}%\")\n",
        "        print(f\"   üíæ Model saved as 'best_plant_disease_model.pth'\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"   ‚è≥ No improvement. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\nüõë Early stopping triggered after {epoch+1} epochs\")\n",
        "        print(f\"   Best validation accuracy achieved: {best_val_acc:.2f}%\")\n",
        "        break\n",
        "\n",
        "    # Add a small delay for better visualization\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(f\"\\nüéâ TRAINING COMPLETED!\")\n",
        "print(f\"   üèÜ Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"   üìÅ Best model saved as: 'best_plant_disease_model.pth'\")"
      ],
      "metadata": {
        "id": "mGC0ezH_oe-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df5d5e3-92d5-48f8-82ce-2576a0165cb4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 1/5\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä EPOCH 1 SUMMARY:\n",
            "   üîµ Train - Loss: 0.9440, Accuracy: 92.71%\n",
            "   üü¢ Valid - Loss: 0.7621, Accuracy: 97.95%\n",
            "   ‚≠ê NEW BEST MODEL! Validation Accuracy: 97.95%\n",
            "   üíæ Model saved as 'best_plant_disease_model.pth'\n",
            "\n",
            "============================================================\n",
            "EPOCH 2/5\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä EPOCH 2 SUMMARY:\n",
            "   üîµ Train - Loss: 0.7814, Accuracy: 97.76%\n",
            "   üü¢ Valid - Loss: 0.7208, Accuracy: 98.82%\n",
            "   ‚≠ê NEW BEST MODEL! Validation Accuracy: 98.82%\n",
            "   üíæ Model saved as 'best_plant_disease_model.pth'\n",
            "\n",
            "============================================================\n",
            "EPOCH 3/5\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä EPOCH 3 SUMMARY:\n",
            "   üîµ Train - Loss: 0.7575, Accuracy: 98.36%\n",
            "   üü¢ Valid - Loss: 0.7312, Accuracy: 98.80%\n",
            "   ‚è≥ No improvement. Patience: 1/5\n",
            "\n",
            "============================================================\n",
            "EPOCH 4/5\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä EPOCH 4 SUMMARY:\n",
            "   üîµ Train - Loss: 0.7455, Accuracy: 98.67%\n",
            "   üü¢ Valid - Loss: 0.7249, Accuracy: 98.57%\n",
            "   ‚è≥ No improvement. Patience: 2/5\n",
            "\n",
            "============================================================\n",
            "EPOCH 5/5\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä EPOCH 5 SUMMARY:\n",
            "   üîµ Train - Loss: 0.7331, Accuracy: 99.00%\n",
            "   üü¢ Valid - Loss: 0.7084, Accuracy: 98.99%\n",
            "   ‚≠ê NEW BEST MODEL! Validation Accuracy: 98.99%\n",
            "   üíæ Model saved as 'best_plant_disease_model.pth'\n",
            "\n",
            "üéâ TRAINING COMPLETED!\n",
            "   üèÜ Best validation accuracy: 98.99%\n",
            "   üìÅ Best model saved as: 'best_plant_disease_model.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "\n",
        "# Disable gradient calculation (faster inference)\n",
        "with torch.no_grad():\n",
        "    for images, labels in valid_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU/CPU\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Update accuracy calculation\n",
        "        correct += torch.sum(preds == labels).item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "# Compute validation accuracy\n",
        "valid_acc = 100.0 * correct / total\n",
        "print(f\" Validation Accuracy: {valid_acc:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "z63nKOeuwPcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e828dbc0-1e55-4a3d-c35e-e5aabd46838e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Accuracy: 98.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"potato_disease_model.pth\")\n"
      ],
      "metadata": {
        "id": "oflY9k-AwmGV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"potato_disease_model.pth\"))\n"
      ],
      "metadata": {
        "id": "la7BYBeCwnc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8845cf67-0b79-4c86-daed-bd21e786e67f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Assuming device is already defined (e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "# Assuming valid_loader is already defined\n",
        "# Assuming class_names is already defined with 38 class names\n",
        "\n",
        "# Placeholder for class_names - REPLACE WITH YOUR ACTUAL LIST OF 38 CLASS NAMES\n",
        "class_names = [f'class_{i}' for i in range(38)]\n",
        "\n",
        "\n",
        "# Enhanced final evaluation with test-time augmentation\n",
        "def test_time_augmentation(model, image, device, num_augmentations=5):\n",
        "    \"\"\"Apply test-time augmentation for better predictions\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    # Original prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        predictions.append(torch.softmax(output, dim=1))\n",
        "\n",
        "    # Augmented predictions\n",
        "    for _ in range(num_augmentations):\n",
        "        # Apply random transformations\n",
        "        aug_transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(degrees=5),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Convert back to PIL and apply augmentation\n",
        "        img_pil = transforms.ToPILImage()(image.squeeze(0).cpu())\n",
        "        aug_img = aug_transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(aug_img)\n",
        "            predictions.append(torch.softmax(output, dim=1))\n",
        "\n",
        "    # Average all predictions\n",
        "    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n",
        "    return avg_prediction\n",
        "\n",
        "# Load best model and evaluate\n",
        "# Re-define the model architecture to match the saved state_dict\n",
        "model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1) # Use weights instead of pretrained\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(512, len(class_names)) # Ensure the output layer size matches the number of classes (38)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Print state dict keys and shapes for comparison (optional, but good for debugging)\n",
        "print(\"Model state_dict keys and shapes:\")\n",
        "for key, value in model.state_dict().items():\n",
        "    print(f\"{key}: {value.shape}\")\n",
        "\n",
        "saved_state_dict = torch.load(\"best_plant_disease_model.pth\")\n",
        "print(\"\\nSaved state_dict keys and shapes:\")\n",
        "for key, value in saved_state_dict.items():\n",
        "    print(f\"{key}: {value.shape}\")\n",
        "\n",
        "\n",
        "model.load_state_dict(saved_state_dict)\n",
        "model.eval()\n",
        "\n",
        "correct, total = 0, 0\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in valid_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Use test-time augmentation for better accuracy\n",
        "        batch_predictions = []\n",
        "        for i in range(images.size(0)):\n",
        "            single_image = images[i:i+1]\n",
        "            avg_pred = test_time_augmentation(model, single_image, device)\n",
        "            batch_predictions.append(avg_pred)\n",
        "\n",
        "        batch_predictions = torch.cat(batch_predictions, dim=0)\n",
        "        _, preds = torch.max(batch_predictions, 1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_predictions.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "final_accuracy = 100.0 * correct / total\n",
        "print(f\"Final Test Accuracy with TTA: {final_accuracy:.2f}%\")\n",
        "\n",
        "# Classification report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(all_labels, all_predictions, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZdtQvn8EoGo",
        "outputId": "a4c4b4cd-6b11-4cd2-a298-387713ecb2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state_dict keys and shapes:\n",
            "features.0.0.weight: torch.Size([40, 3, 3, 3])\n",
            "features.0.1.weight: torch.Size([40])\n",
            "features.0.1.bias: torch.Size([40])\n",
            "features.0.1.running_mean: torch.Size([40])\n",
            "features.0.1.running_var: torch.Size([40])\n",
            "features.0.1.num_batches_tracked: torch.Size([])\n",
            "features.1.0.block.0.0.weight: torch.Size([40, 1, 3, 3])\n",
            "features.1.0.block.0.1.weight: torch.Size([40])\n",
            "features.1.0.block.0.1.bias: torch.Size([40])\n",
            "features.1.0.block.0.1.running_mean: torch.Size([40])\n",
            "features.1.0.block.0.1.running_var: torch.Size([40])\n",
            "features.1.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.1.0.block.1.fc1.weight: torch.Size([10, 40, 1, 1])\n",
            "features.1.0.block.1.fc1.bias: torch.Size([10])\n",
            "features.1.0.block.1.fc2.weight: torch.Size([40, 10, 1, 1])\n",
            "features.1.0.block.1.fc2.bias: torch.Size([40])\n",
            "features.1.0.block.2.0.weight: torch.Size([24, 40, 1, 1])\n",
            "features.1.0.block.2.1.weight: torch.Size([24])\n",
            "features.1.0.block.2.1.bias: torch.Size([24])\n",
            "features.1.0.block.2.1.running_mean: torch.Size([24])\n",
            "features.1.0.block.2.1.running_var: torch.Size([24])\n",
            "features.1.0.block.2.1.num_batches_tracked: torch.Size([])\n",
            "features.1.1.block.0.0.weight: torch.Size([24, 1, 3, 3])\n",
            "features.1.1.block.0.1.weight: torch.Size([24])\n",
            "features.1.1.block.0.1.bias: torch.Size([24])\n",
            "features.1.1.block.0.1.running_mean: torch.Size([24])\n",
            "features.1.1.block.0.1.running_var: torch.Size([24])\n",
            "features.1.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.1.1.block.1.fc1.weight: torch.Size([6, 24, 1, 1])\n",
            "features.1.1.block.1.fc1.bias: torch.Size([6])\n",
            "features.1.1.block.1.fc2.weight: torch.Size([24, 6, 1, 1])\n",
            "features.1.1.block.1.fc2.bias: torch.Size([24])\n",
            "features.1.1.block.2.0.weight: torch.Size([24, 24, 1, 1])\n",
            "features.1.1.block.2.1.weight: torch.Size([24])\n",
            "features.1.1.block.2.1.bias: torch.Size([24])\n",
            "features.1.1.block.2.1.running_mean: torch.Size([24])\n",
            "features.1.1.block.2.1.running_var: torch.Size([24])\n",
            "features.1.1.block.2.1.num_batches_tracked: torch.Size([])\n",
            "features.2.0.block.0.0.weight: torch.Size([144, 24, 1, 1])\n",
            "features.2.0.block.0.1.weight: torch.Size([144])\n",
            "features.2.0.block.0.1.bias: torch.Size([144])\n",
            "features.2.0.block.0.1.running_mean: torch.Size([144])\n",
            "features.2.0.block.0.1.running_var: torch.Size([144])\n",
            "features.2.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.2.0.block.1.0.weight: torch.Size([144, 1, 3, 3])\n",
            "features.2.0.block.1.1.weight: torch.Size([144])\n",
            "features.2.0.block.1.1.bias: torch.Size([144])\n",
            "features.2.0.block.1.1.running_mean: torch.Size([144])\n",
            "features.2.0.block.1.1.running_var: torch.Size([144])\n",
            "features.2.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.2.0.block.2.fc1.weight: torch.Size([6, 144, 1, 1])\n",
            "features.2.0.block.2.fc1.bias: torch.Size([6])\n",
            "features.2.0.block.2.fc2.weight: torch.Size([144, 6, 1, 1])\n",
            "features.2.0.block.2.fc2.bias: torch.Size([144])\n",
            "features.2.0.block.3.0.weight: torch.Size([32, 144, 1, 1])\n",
            "features.2.0.block.3.1.weight: torch.Size([32])\n",
            "features.2.0.block.3.1.bias: torch.Size([32])\n",
            "features.2.0.block.3.1.running_mean: torch.Size([32])\n",
            "features.2.0.block.3.1.running_var: torch.Size([32])\n",
            "features.2.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.2.1.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
            "features.2.1.block.0.1.weight: torch.Size([192])\n",
            "features.2.1.block.0.1.bias: torch.Size([192])\n",
            "features.2.1.block.0.1.running_mean: torch.Size([192])\n",
            "features.2.1.block.0.1.running_var: torch.Size([192])\n",
            "features.2.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.2.1.block.1.0.weight: torch.Size([192, 1, 3, 3])\n",
            "features.2.1.block.1.1.weight: torch.Size([192])\n",
            "features.2.1.block.1.1.bias: torch.Size([192])\n",
            "features.2.1.block.1.1.running_mean: torch.Size([192])\n",
            "features.2.1.block.1.1.running_var: torch.Size([192])\n",
            "features.2.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.2.1.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
            "features.2.1.block.2.fc1.bias: torch.Size([8])\n",
            "features.2.1.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
            "features.2.1.block.2.fc2.bias: torch.Size([192])\n",
            "features.2.1.block.3.0.weight: torch.Size([32, 192, 1, 1])\n",
            "features.2.1.block.3.1.weight: torch.Size([32])\n",
            "features.2.1.block.3.1.bias: torch.Size([32])\n",
            "features.2.1.block.3.1.running_mean: torch.Size([32])\n",
            "features.2.1.block.3.1.running_var: torch.Size([32])\n",
            "features.2.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.2.2.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
            "features.2.2.block.0.1.weight: torch.Size([192])\n",
            "features.2.2.block.0.1.bias: torch.Size([192])\n",
            "features.2.2.block.0.1.running_mean: torch.Size([192])\n",
            "features.2.2.block.0.1.running_var: torch.Size([192])\n",
            "features.2.2.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.2.2.block.1.0.weight: torch.Size([192, 1, 3, 3])\n",
            "features.2.2.block.1.1.weight: torch.Size([192])\n",
            "features.2.2.block.1.1.bias: torch.Size([192])\n",
            "features.2.2.block.1.1.running_mean: torch.Size([192])\n",
            "features.2.2.block.1.1.running_var: torch.Size([192])\n",
            "features.2.2.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.2.2.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
            "features.2.2.block.2.fc1.bias: torch.Size([8])\n",
            "features.2.2.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
            "features.2.2.block.2.fc2.bias: torch.Size([192])\n",
            "features.2.2.block.3.0.weight: torch.Size([32, 192, 1, 1])\n",
            "features.2.2.block.3.1.weight: torch.Size([32])\n",
            "features.2.2.block.3.1.bias: torch.Size([32])\n",
            "features.2.2.block.3.1.running_mean: torch.Size([32])\n",
            "features.2.2.block.3.1.running_var: torch.Size([32])\n",
            "features.2.2.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.3.0.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
            "features.3.0.block.0.1.weight: torch.Size([192])\n",
            "features.3.0.block.0.1.bias: torch.Size([192])\n",
            "features.3.0.block.0.1.running_mean: torch.Size([192])\n",
            "features.3.0.block.0.1.running_var: torch.Size([192])\n",
            "features.3.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.3.0.block.1.0.weight: torch.Size([192, 1, 5, 5])\n",
            "features.3.0.block.1.1.weight: torch.Size([192])\n",
            "features.3.0.block.1.1.bias: torch.Size([192])\n",
            "features.3.0.block.1.1.running_mean: torch.Size([192])\n",
            "features.3.0.block.1.1.running_var: torch.Size([192])\n",
            "features.3.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.3.0.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
            "features.3.0.block.2.fc1.bias: torch.Size([8])\n",
            "features.3.0.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
            "features.3.0.block.2.fc2.bias: torch.Size([192])\n",
            "features.3.0.block.3.0.weight: torch.Size([48, 192, 1, 1])\n",
            "features.3.0.block.3.1.weight: torch.Size([48])\n",
            "features.3.0.block.3.1.bias: torch.Size([48])\n",
            "features.3.0.block.3.1.running_mean: torch.Size([48])\n",
            "features.3.0.block.3.1.running_var: torch.Size([48])\n",
            "features.3.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.3.1.block.0.0.weight: torch.Size([288, 48, 1, 1])\n",
            "features.3.1.block.0.1.weight: torch.Size([288])\n",
            "features.3.1.block.0.1.bias: torch.Size([288])\n",
            "features.3.1.block.0.1.running_mean: torch.Size([288])\n",
            "features.3.1.block.0.1.running_var: torch.Size([288])\n",
            "features.3.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.3.1.block.1.0.weight: torch.Size([288, 1, 5, 5])\n",
            "features.3.1.block.1.1.weight: torch.Size([288])\n",
            "features.3.1.block.1.1.bias: torch.Size([288])\n",
            "features.3.1.block.1.1.running_mean: torch.Size([288])\n",
            "features.3.1.block.1.1.running_var: torch.Size([288])\n",
            "features.3.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.3.1.block.2.fc1.weight: torch.Size([12, 288, 1, 1])\n",
            "features.3.1.block.2.fc1.bias: torch.Size([12])\n",
            "features.3.1.block.2.fc2.weight: torch.Size([288, 12, 1, 1])\n",
            "features.3.1.block.2.fc2.bias: torch.Size([288])\n",
            "features.3.1.block.3.0.weight: torch.Size([48, 288, 1, 1])\n",
            "features.3.1.block.3.1.weight: torch.Size([48])\n",
            "features.3.1.block.3.1.bias: torch.Size([48])\n",
            "features.3.1.block.3.1.running_mean: torch.Size([48])\n",
            "features.3.1.block.3.1.running_var: torch.Size([48])\n",
            "features.3.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.3.2.block.0.0.weight: torch.Size([288, 48, 1, 1])\n",
            "features.3.2.block.0.1.weight: torch.Size([288])\n",
            "features.3.2.block.0.1.bias: torch.Size([288])\n",
            "features.3.2.block.0.1.running_mean: torch.Size([288])\n",
            "features.3.2.block.0.1.running_var: torch.Size([288])\n",
            "features.3.2.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.3.2.block.1.0.weight: torch.Size([288, 1, 5, 5])\n",
            "features.3.2.block.1.1.weight: torch.Size([288])\n",
            "features.3.2.block.1.1.bias: torch.Size([288])\n",
            "features.3.2.block.1.1.running_mean: torch.Size([288])\n",
            "features.3.2.block.1.1.running_var: torch.Size([288])\n",
            "features.3.2.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.3.2.block.2.fc1.weight: torch.Size([12, 288, 1, 1])\n",
            "features.3.2.block.2.fc1.bias: torch.Size([12])\n",
            "features.3.2.block.2.fc2.weight: torch.Size([288, 12, 1, 1])\n",
            "features.3.2.block.2.fc2.bias: torch.Size([288])\n",
            "features.3.2.block.3.0.weight: torch.Size([48, 288, 1, 1])\n",
            "features.3.2.block.3.1.weight: torch.Size([48])\n",
            "features.3.2.block.3.1.bias: torch.Size([48])\n",
            "features.3.2.block.3.1.running_mean: torch.Size([48])\n",
            "features.3.2.block.3.1.running_var: torch.Size([48])\n",
            "features.3.2.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.4.0.block.0.0.weight: torch.Size([288, 48, 1, 1])\n",
            "features.4.0.block.0.1.weight: torch.Size([288])\n",
            "features.4.0.block.0.1.bias: torch.Size([288])\n",
            "features.4.0.block.0.1.running_mean: torch.Size([288])\n",
            "features.4.0.block.0.1.running_var: torch.Size([288])\n",
            "features.4.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.4.0.block.1.0.weight: torch.Size([288, 1, 3, 3])\n",
            "features.4.0.block.1.1.weight: torch.Size([288])\n",
            "features.4.0.block.1.1.bias: torch.Size([288])\n",
            "features.4.0.block.1.1.running_mean: torch.Size([288])\n",
            "features.4.0.block.1.1.running_var: torch.Size([288])\n",
            "features.4.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.4.0.block.2.fc1.weight: torch.Size([12, 288, 1, 1])\n",
            "features.4.0.block.2.fc1.bias: torch.Size([12])\n",
            "features.4.0.block.2.fc2.weight: torch.Size([288, 12, 1, 1])\n",
            "features.4.0.block.2.fc2.bias: torch.Size([288])\n",
            "features.4.0.block.3.0.weight: torch.Size([96, 288, 1, 1])\n",
            "features.4.0.block.3.1.weight: torch.Size([96])\n",
            "features.4.0.block.3.1.bias: torch.Size([96])\n",
            "features.4.0.block.3.1.running_mean: torch.Size([96])\n",
            "features.4.0.block.3.1.running_var: torch.Size([96])\n",
            "features.4.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.4.1.block.0.0.weight: torch.Size([576, 96, 1, 1])\n",
            "features.4.1.block.0.1.weight: torch.Size([576])\n",
            "features.4.1.block.0.1.bias: torch.Size([576])\n",
            "features.4.1.block.0.1.running_mean: torch.Size([576])\n",
            "features.4.1.block.0.1.running_var: torch.Size([576])\n",
            "features.4.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.4.1.block.1.0.weight: torch.Size([576, 1, 3, 3])\n",
            "features.4.1.block.1.1.weight: torch.Size([576])\n",
            "features.4.1.block.1.1.bias: torch.Size([576])\n",
            "features.4.1.block.1.1.running_mean: torch.Size([576])\n",
            "features.4.1.block.1.1.running_var: torch.Size([576])\n",
            "features.4.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.4.1.block.2.fc1.weight: torch.Size([24, 576, 1, 1])\n",
            "features.4.1.block.2.fc1.bias: torch.Size([24])\n",
            "features.4.1.block.2.fc2.weight: torch.Size([576, 24, 1, 1])\n",
            "features.4.1.block.2.fc2.bias: torch.Size([576])\n",
            "features.4.1.block.3.0.weight: torch.Size([96, 576, 1, 1])\n",
            "features.4.1.block.3.1.weight: torch.Size([96])\n",
            "features.4.1.block.3.1.bias: torch.Size([96])\n",
            "features.4.1.block.3.1.running_mean: torch.Size([96])\n",
            "features.4.1.block.3.1.running_var: torch.Size([96])\n",
            "features.4.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.4.2.block.0.0.weight: torch.Size([576, 96, 1, 1])\n",
            "features.4.2.block.0.1.weight: torch.Size([576])\n",
            "features.4.2.block.0.1.bias: torch.Size([576])\n",
            "features.4.2.block.0.1.running_mean: torch.Size([576])\n",
            "features.4.2.block.0.1.running_var: torch.Size([576])\n",
            "features.4.2.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.4.2.block.1.0.weight: torch.Size([576, 1, 3, 3])\n",
            "features.4.2.block.1.1.weight: torch.Size([576])\n",
            "features.4.2.block.1.1.bias: torch.Size([576])\n",
            "features.4.2.block.1.1.running_mean: torch.Size([576])\n",
            "features.4.2.block.1.1.running_var: torch.Size([576])\n",
            "features.4.2.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.4.2.block.2.fc1.weight: torch.Size([24, 576, 1, 1])\n",
            "features.4.2.block.2.fc1.bias: torch.Size([24])\n",
            "features.4.2.block.2.fc2.weight: torch.Size([576, 24, 1, 1])\n",
            "features.4.2.block.2.fc2.bias: torch.Size([576])\n",
            "features.4.2.block.3.0.weight: torch.Size([96, 576, 1, 1])\n",
            "features.4.2.block.3.1.weight: torch.Size([96])\n",
            "features.4.2.block.3.1.bias: torch.Size([96])\n",
            "features.4.2.block.3.1.running_mean: torch.Size([96])\n",
            "features.4.2.block.3.1.running_var: torch.Size([96])\n",
            "features.4.2.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.4.3.block.0.0.weight: torch.Size([576, 96, 1, 1])\n",
            "features.4.3.block.0.1.weight: torch.Size([576])\n",
            "features.4.3.block.0.1.bias: torch.Size([576])\n",
            "features.4.3.block.0.1.running_mean: torch.Size([576])\n",
            "features.4.3.block.0.1.running_var: torch.Size([576])\n",
            "features.4.3.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.4.3.block.1.0.weight: torch.Size([576, 1, 3, 3])\n",
            "features.4.3.block.1.1.weight: torch.Size([576])\n",
            "features.4.3.block.1.1.bias: torch.Size([576])\n",
            "features.4.3.block.1.1.running_mean: torch.Size([576])\n",
            "features.4.3.block.1.1.running_var: torch.Size([576])\n",
            "features.4.3.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.4.3.block.2.fc1.weight: torch.Size([24, 576, 1, 1])\n",
            "features.4.3.block.2.fc1.bias: torch.Size([24])\n",
            "features.4.3.block.2.fc2.weight: torch.Size([576, 24, 1, 1])\n",
            "features.4.3.block.2.fc2.bias: torch.Size([576])\n",
            "features.4.3.block.3.0.weight: torch.Size([96, 576, 1, 1])\n",
            "features.4.3.block.3.1.weight: torch.Size([96])\n",
            "features.4.3.block.3.1.bias: torch.Size([96])\n",
            "features.4.3.block.3.1.running_mean: torch.Size([96])\n",
            "features.4.3.block.3.1.running_var: torch.Size([96])\n",
            "features.4.3.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.4.4.block.0.0.weight: torch.Size([576, 96, 1, 1])\n",
            "features.4.4.block.0.1.weight: torch.Size([576])\n",
            "features.4.4.block.0.1.bias: torch.Size([576])\n",
            "features.4.4.block.0.1.running_mean: torch.Size([576])\n",
            "features.4.4.block.0.1.running_var: torch.Size([576])\n",
            "features.4.4.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.4.4.block.1.0.weight: torch.Size([576, 1, 3, 3])\n",
            "features.4.4.block.1.1.weight: torch.Size([576])\n",
            "features.4.4.block.1.1.bias: torch.Size([576])\n",
            "features.4.4.block.1.1.running_mean: torch.Size([576])\n",
            "features.4.4.block.1.1.running_var: torch.Size([576])\n",
            "features.4.4.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.4.4.block.2.fc1.weight: torch.Size([24, 576, 1, 1])\n",
            "features.4.4.block.2.fc1.bias: torch.Size([24])\n",
            "features.4.4.block.2.fc2.weight: torch.Size([576, 24, 1, 1])\n",
            "features.4.4.block.2.fc2.bias: torch.Size([576])\n",
            "features.4.4.block.3.0.weight: torch.Size([96, 576, 1, 1])\n",
            "features.4.4.block.3.1.weight: torch.Size([96])\n",
            "features.4.4.block.3.1.bias: torch.Size([96])\n",
            "features.4.4.block.3.1.running_mean: torch.Size([96])\n",
            "features.4.4.block.3.1.running_var: torch.Size([96])\n",
            "features.4.4.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.5.0.block.0.0.weight: torch.Size([576, 96, 1, 1])\n",
            "features.5.0.block.0.1.weight: torch.Size([576])\n",
            "features.5.0.block.0.1.bias: torch.Size([576])\n",
            "features.5.0.block.0.1.running_mean: torch.Size([576])\n",
            "features.5.0.block.0.1.running_var: torch.Size([576])\n",
            "features.5.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.5.0.block.1.0.weight: torch.Size([576, 1, 5, 5])\n",
            "features.5.0.block.1.1.weight: torch.Size([576])\n",
            "features.5.0.block.1.1.bias: torch.Size([576])\n",
            "features.5.0.block.1.1.running_mean: torch.Size([576])\n",
            "features.5.0.block.1.1.running_var: torch.Size([576])\n",
            "features.5.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.5.0.block.2.fc1.weight: torch.Size([24, 576, 1, 1])\n",
            "features.5.0.block.2.fc1.bias: torch.Size([24])\n",
            "features.5.0.block.2.fc2.weight: torch.Size([576, 24, 1, 1])\n",
            "features.5.0.block.2.fc2.bias: torch.Size([576])\n",
            "features.5.0.block.3.0.weight: torch.Size([136, 576, 1, 1])\n",
            "features.5.0.block.3.1.weight: torch.Size([136])\n",
            "features.5.0.block.3.1.bias: torch.Size([136])\n",
            "features.5.0.block.3.1.running_mean: torch.Size([136])\n",
            "features.5.0.block.3.1.running_var: torch.Size([136])\n",
            "features.5.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.5.1.block.0.0.weight: torch.Size([816, 136, 1, 1])\n",
            "features.5.1.block.0.1.weight: torch.Size([816])\n",
            "features.5.1.block.0.1.bias: torch.Size([816])\n",
            "features.5.1.block.0.1.running_mean: torch.Size([816])\n",
            "features.5.1.block.0.1.running_var: torch.Size([816])\n",
            "features.5.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.5.1.block.1.0.weight: torch.Size([816, 1, 5, 5])\n",
            "features.5.1.block.1.1.weight: torch.Size([816])\n",
            "features.5.1.block.1.1.bias: torch.Size([816])\n",
            "features.5.1.block.1.1.running_mean: torch.Size([816])\n",
            "features.5.1.block.1.1.running_var: torch.Size([816])\n",
            "features.5.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.5.1.block.2.fc1.weight: torch.Size([34, 816, 1, 1])\n",
            "features.5.1.block.2.fc1.bias: torch.Size([34])\n",
            "features.5.1.block.2.fc2.weight: torch.Size([816, 34, 1, 1])\n",
            "features.5.1.block.2.fc2.bias: torch.Size([816])\n",
            "features.5.1.block.3.0.weight: torch.Size([136, 816, 1, 1])\n",
            "features.5.1.block.3.1.weight: torch.Size([136])\n",
            "features.5.1.block.3.1.bias: torch.Size([136])\n",
            "features.5.1.block.3.1.running_mean: torch.Size([136])\n",
            "features.5.1.block.3.1.running_var: torch.Size([136])\n",
            "features.5.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.5.2.block.0.0.weight: torch.Size([816, 136, 1, 1])\n",
            "features.5.2.block.0.1.weight: torch.Size([816])\n",
            "features.5.2.block.0.1.bias: torch.Size([816])\n",
            "features.5.2.block.0.1.running_mean: torch.Size([816])\n",
            "features.5.2.block.0.1.running_var: torch.Size([816])\n",
            "features.5.2.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.5.2.block.1.0.weight: torch.Size([816, 1, 5, 5])\n",
            "features.5.2.block.1.1.weight: torch.Size([816])\n",
            "features.5.2.block.1.1.bias: torch.Size([816])\n",
            "features.5.2.block.1.1.running_mean: torch.Size([816])\n",
            "features.5.2.block.1.1.running_var: torch.Size([816])\n",
            "features.5.2.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.5.2.block.2.fc1.weight: torch.Size([34, 816, 1, 1])\n",
            "features.5.2.block.2.fc1.bias: torch.Size([34])\n",
            "features.5.2.block.2.fc2.weight: torch.Size([816, 34, 1, 1])\n",
            "features.5.2.block.2.fc2.bias: torch.Size([816])\n",
            "features.5.2.block.3.0.weight: torch.Size([136, 816, 1, 1])\n",
            "features.5.2.block.3.1.weight: torch.Size([136])\n",
            "features.5.2.block.3.1.bias: torch.Size([136])\n",
            "features.5.2.block.3.1.running_mean: torch.Size([136])\n",
            "features.5.2.block.3.1.running_var: torch.Size([136])\n",
            "features.5.2.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.5.3.block.0.0.weight: torch.Size([816, 136, 1, 1])\n",
            "features.5.3.block.0.1.weight: torch.Size([816])\n",
            "features.5.3.block.0.1.bias: torch.Size([816])\n",
            "features.5.3.block.0.1.running_mean: torch.Size([816])\n",
            "features.5.3.block.0.1.running_var: torch.Size([816])\n",
            "features.5.3.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.5.3.block.1.0.weight: torch.Size([816, 1, 5, 5])\n",
            "features.5.3.block.1.1.weight: torch.Size([816])\n",
            "features.5.3.block.1.1.bias: torch.Size([816])\n",
            "features.5.3.block.1.1.running_mean: torch.Size([816])\n",
            "features.5.3.block.1.1.running_var: torch.Size([816])\n",
            "features.5.3.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.5.3.block.2.fc1.weight: torch.Size([34, 816, 1, 1])\n",
            "features.5.3.block.2.fc1.bias: torch.Size([34])\n",
            "features.5.3.block.2.fc2.weight: torch.Size([816, 34, 1, 1])\n",
            "features.5.3.block.2.fc2.bias: torch.Size([816])\n",
            "features.5.3.block.3.0.weight: torch.Size([136, 816, 1, 1])\n",
            "features.5.3.block.3.1.weight: torch.Size([136])\n",
            "features.5.3.block.3.1.bias: torch.Size([136])\n",
            "features.5.3.block.3.1.running_mean: torch.Size([136])\n",
            "features.5.3.block.3.1.running_var: torch.Size([136])\n",
            "features.5.3.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.5.4.block.0.0.weight: torch.Size([816, 136, 1, 1])\n",
            "features.5.4.block.0.1.weight: torch.Size([816])\n",
            "features.5.4.block.0.1.bias: torch.Size([816])\n",
            "features.5.4.block.0.1.running_mean: torch.Size([816])\n",
            "features.5.4.block.0.1.running_var: torch.Size([816])\n",
            "features.5.4.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.5.4.block.1.0.weight: torch.Size([816, 1, 5, 5])\n",
            "features.5.4.block.1.1.weight: torch.Size([816])\n",
            "features.5.4.block.1.1.bias: torch.Size([816])\n",
            "features.5.4.block.1.1.running_mean: torch.Size([816])\n",
            "features.5.4.block.1.1.running_var: torch.Size([816])\n",
            "features.5.4.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.5.4.block.2.fc1.weight: torch.Size([34, 816, 1, 1])\n",
            "features.5.4.block.2.fc1.bias: torch.Size([34])\n",
            "features.5.4.block.2.fc2.weight: torch.Size([816, 34, 1, 1])\n",
            "features.5.4.block.2.fc2.bias: torch.Size([816])\n",
            "features.5.4.block.3.0.weight: torch.Size([136, 816, 1, 1])\n",
            "features.5.4.block.3.1.weight: torch.Size([136])\n",
            "features.5.4.block.3.1.bias: torch.Size([136])\n",
            "features.5.4.block.3.1.running_mean: torch.Size([136])\n",
            "features.5.4.block.3.1.running_var: torch.Size([136])\n",
            "features.5.4.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.0.block.0.0.weight: torch.Size([816, 136, 1, 1])\n",
            "features.6.0.block.0.1.weight: torch.Size([816])\n",
            "features.6.0.block.0.1.bias: torch.Size([816])\n",
            "features.6.0.block.0.1.running_mean: torch.Size([816])\n",
            "features.6.0.block.0.1.running_var: torch.Size([816])\n",
            "features.6.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.0.block.1.0.weight: torch.Size([816, 1, 5, 5])\n",
            "features.6.0.block.1.1.weight: torch.Size([816])\n",
            "features.6.0.block.1.1.bias: torch.Size([816])\n",
            "features.6.0.block.1.1.running_mean: torch.Size([816])\n",
            "features.6.0.block.1.1.running_var: torch.Size([816])\n",
            "features.6.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.0.block.2.fc1.weight: torch.Size([34, 816, 1, 1])\n",
            "features.6.0.block.2.fc1.bias: torch.Size([34])\n",
            "features.6.0.block.2.fc2.weight: torch.Size([816, 34, 1, 1])\n",
            "features.6.0.block.2.fc2.bias: torch.Size([816])\n",
            "features.6.0.block.3.0.weight: torch.Size([232, 816, 1, 1])\n",
            "features.6.0.block.3.1.weight: torch.Size([232])\n",
            "features.6.0.block.3.1.bias: torch.Size([232])\n",
            "features.6.0.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.0.block.3.1.running_var: torch.Size([232])\n",
            "features.6.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.1.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.6.1.block.0.1.weight: torch.Size([1392])\n",
            "features.6.1.block.0.1.bias: torch.Size([1392])\n",
            "features.6.1.block.0.1.running_mean: torch.Size([1392])\n",
            "features.6.1.block.0.1.running_var: torch.Size([1392])\n",
            "features.6.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.1.block.1.0.weight: torch.Size([1392, 1, 5, 5])\n",
            "features.6.1.block.1.1.weight: torch.Size([1392])\n",
            "features.6.1.block.1.1.bias: torch.Size([1392])\n",
            "features.6.1.block.1.1.running_mean: torch.Size([1392])\n",
            "features.6.1.block.1.1.running_var: torch.Size([1392])\n",
            "features.6.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.1.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.6.1.block.2.fc1.bias: torch.Size([58])\n",
            "features.6.1.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.6.1.block.2.fc2.bias: torch.Size([1392])\n",
            "features.6.1.block.3.0.weight: torch.Size([232, 1392, 1, 1])\n",
            "features.6.1.block.3.1.weight: torch.Size([232])\n",
            "features.6.1.block.3.1.bias: torch.Size([232])\n",
            "features.6.1.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.1.block.3.1.running_var: torch.Size([232])\n",
            "features.6.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.2.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.6.2.block.0.1.weight: torch.Size([1392])\n",
            "features.6.2.block.0.1.bias: torch.Size([1392])\n",
            "features.6.2.block.0.1.running_mean: torch.Size([1392])\n",
            "features.6.2.block.0.1.running_var: torch.Size([1392])\n",
            "features.6.2.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.2.block.1.0.weight: torch.Size([1392, 1, 5, 5])\n",
            "features.6.2.block.1.1.weight: torch.Size([1392])\n",
            "features.6.2.block.1.1.bias: torch.Size([1392])\n",
            "features.6.2.block.1.1.running_mean: torch.Size([1392])\n",
            "features.6.2.block.1.1.running_var: torch.Size([1392])\n",
            "features.6.2.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.2.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.6.2.block.2.fc1.bias: torch.Size([58])\n",
            "features.6.2.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.6.2.block.2.fc2.bias: torch.Size([1392])\n",
            "features.6.2.block.3.0.weight: torch.Size([232, 1392, 1, 1])\n",
            "features.6.2.block.3.1.weight: torch.Size([232])\n",
            "features.6.2.block.3.1.bias: torch.Size([232])\n",
            "features.6.2.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.2.block.3.1.running_var: torch.Size([232])\n",
            "features.6.2.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.3.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.6.3.block.0.1.weight: torch.Size([1392])\n",
            "features.6.3.block.0.1.bias: torch.Size([1392])\n",
            "features.6.3.block.0.1.running_mean: torch.Size([1392])\n",
            "features.6.3.block.0.1.running_var: torch.Size([1392])\n",
            "features.6.3.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.3.block.1.0.weight: torch.Size([1392, 1, 5, 5])\n",
            "features.6.3.block.1.1.weight: torch.Size([1392])\n",
            "features.6.3.block.1.1.bias: torch.Size([1392])\n",
            "features.6.3.block.1.1.running_mean: torch.Size([1392])\n",
            "features.6.3.block.1.1.running_var: torch.Size([1392])\n",
            "features.6.3.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.3.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.6.3.block.2.fc1.bias: torch.Size([58])\n",
            "features.6.3.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.6.3.block.2.fc2.bias: torch.Size([1392])\n",
            "features.6.3.block.3.0.weight: torch.Size([232, 1392, 1, 1])\n",
            "features.6.3.block.3.1.weight: torch.Size([232])\n",
            "features.6.3.block.3.1.bias: torch.Size([232])\n",
            "features.6.3.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.3.block.3.1.running_var: torch.Size([232])\n",
            "features.6.3.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.4.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.6.4.block.0.1.weight: torch.Size([1392])\n",
            "features.6.4.block.0.1.bias: torch.Size([1392])\n",
            "features.6.4.block.0.1.running_mean: torch.Size([1392])\n",
            "features.6.4.block.0.1.running_var: torch.Size([1392])\n",
            "features.6.4.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.4.block.1.0.weight: torch.Size([1392, 1, 5, 5])\n",
            "features.6.4.block.1.1.weight: torch.Size([1392])\n",
            "features.6.4.block.1.1.bias: torch.Size([1392])\n",
            "features.6.4.block.1.1.running_mean: torch.Size([1392])\n",
            "features.6.4.block.1.1.running_var: torch.Size([1392])\n",
            "features.6.4.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.4.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.6.4.block.2.fc1.bias: torch.Size([58])\n",
            "features.6.4.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.6.4.block.2.fc2.bias: torch.Size([1392])\n",
            "features.6.4.block.3.0.weight: torch.Size([232, 1392, 1, 1])\n",
            "features.6.4.block.3.1.weight: torch.Size([232])\n",
            "features.6.4.block.3.1.bias: torch.Size([232])\n",
            "features.6.4.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.4.block.3.1.running_var: torch.Size([232])\n",
            "features.6.4.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.5.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.6.5.block.0.1.weight: torch.Size([1392])\n",
            "features.6.5.block.0.1.bias: torch.Size([1392])\n",
            "features.6.5.block.0.1.running_mean: torch.Size([1392])\n",
            "features.6.5.block.0.1.running_var: torch.Size([1392])\n",
            "features.6.5.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.5.block.1.0.weight: torch.Size([1392, 1, 5, 5])\n",
            "features.6.5.block.1.1.weight: torch.Size([1392])\n",
            "features.6.5.block.1.1.bias: torch.Size([1392])\n",
            "features.6.5.block.1.1.running_mean: torch.Size([1392])\n",
            "features.6.5.block.1.1.running_var: torch.Size([1392])\n",
            "features.6.5.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.5.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.6.5.block.2.fc1.bias: torch.Size([58])\n",
            "features.6.5.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.6.5.block.2.fc2.bias: torch.Size([1392])\n",
            "features.6.5.block.3.0.weight: torch.Size([232, 1392, 1, 1])\n",
            "features.6.5.block.3.1.weight: torch.Size([232])\n",
            "features.6.5.block.3.1.bias: torch.Size([232])\n",
            "features.6.5.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.5.block.3.1.running_var: torch.Size([232])\n",
            "features.6.5.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.7.0.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.7.0.block.0.1.weight: torch.Size([1392])\n",
            "features.7.0.block.0.1.bias: torch.Size([1392])\n",
            "features.7.0.block.0.1.running_mean: torch.Size([1392])\n",
            "features.7.0.block.0.1.running_var: torch.Size([1392])\n",
            "features.7.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.7.0.block.1.0.weight: torch.Size([1392, 1, 3, 3])\n",
            "features.7.0.block.1.1.weight: torch.Size([1392])\n",
            "features.7.0.block.1.1.bias: torch.Size([1392])\n",
            "features.7.0.block.1.1.running_mean: torch.Size([1392])\n",
            "features.7.0.block.1.1.running_var: torch.Size([1392])\n",
            "features.7.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.7.0.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.7.0.block.2.fc1.bias: torch.Size([58])\n",
            "features.7.0.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.7.0.block.2.fc2.bias: torch.Size([1392])\n",
            "features.7.0.block.3.0.weight: torch.Size([384, 1392, 1, 1])\n",
            "features.7.0.block.3.1.weight: torch.Size([384])\n",
            "features.7.0.block.3.1.bias: torch.Size([384])\n",
            "features.7.0.block.3.1.running_mean: torch.Size([384])\n",
            "features.7.0.block.3.1.running_var: torch.Size([384])\n",
            "features.7.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.7.1.block.0.0.weight: torch.Size([2304, 384, 1, 1])\n",
            "features.7.1.block.0.1.weight: torch.Size([2304])\n",
            "features.7.1.block.0.1.bias: torch.Size([2304])\n",
            "features.7.1.block.0.1.running_mean: torch.Size([2304])\n",
            "features.7.1.block.0.1.running_var: torch.Size([2304])\n",
            "features.7.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.7.1.block.1.0.weight: torch.Size([2304, 1, 3, 3])\n",
            "features.7.1.block.1.1.weight: torch.Size([2304])\n",
            "features.7.1.block.1.1.bias: torch.Size([2304])\n",
            "features.7.1.block.1.1.running_mean: torch.Size([2304])\n",
            "features.7.1.block.1.1.running_var: torch.Size([2304])\n",
            "features.7.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.7.1.block.2.fc1.weight: torch.Size([96, 2304, 1, 1])\n",
            "features.7.1.block.2.fc1.bias: torch.Size([96])\n",
            "features.7.1.block.2.fc2.weight: torch.Size([2304, 96, 1, 1])\n",
            "features.7.1.block.2.fc2.bias: torch.Size([2304])\n",
            "features.7.1.block.3.0.weight: torch.Size([384, 2304, 1, 1])\n",
            "features.7.1.block.3.1.weight: torch.Size([384])\n",
            "features.7.1.block.3.1.bias: torch.Size([384])\n",
            "features.7.1.block.3.1.running_mean: torch.Size([384])\n",
            "features.7.1.block.3.1.running_var: torch.Size([384])\n",
            "features.7.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.8.0.weight: torch.Size([1536, 384, 1, 1])\n",
            "features.8.1.weight: torch.Size([1536])\n",
            "features.8.1.bias: torch.Size([1536])\n",
            "features.8.1.running_mean: torch.Size([1536])\n",
            "features.8.1.running_var: torch.Size([1536])\n",
            "features.8.1.num_batches_tracked: torch.Size([])\n",
            "classifier.1.weight: torch.Size([512, 1536])\n",
            "classifier.1.bias: torch.Size([512])\n",
            "classifier.4.weight: torch.Size([38, 512])\n",
            "classifier.4.bias: torch.Size([38])\n",
            "\n",
            "Saved state_dict keys and shapes:\n",
            "features.0.0.weight: torch.Size([40, 3, 3, 3])\n",
            "features.0.1.weight: torch.Size([40])\n",
            "features.0.1.bias: torch.Size([40])\n",
            "features.0.1.running_mean: torch.Size([40])\n",
            "features.0.1.running_var: torch.Size([40])\n",
            "features.0.1.num_batches_tracked: torch.Size([])\n",
            "features.1.0.block.0.0.weight: torch.Size([40, 1, 3, 3])\n",
            "features.1.0.block.0.1.weight: torch.Size([40])\n",
            "features.1.0.block.0.1.bias: torch.Size([40])\n",
            "features.1.0.block.0.1.running_mean: torch.Size([40])\n",
            "features.1.0.block.0.1.running_var: torch.Size([40])\n",
            "features.1.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.1.0.block.1.fc1.weight: torch.Size([10, 40, 1, 1])\n",
            "features.1.0.block.1.fc1.bias: torch.Size([10])\n",
            "features.1.0.block.1.fc2.weight: torch.Size([40, 10, 1, 1])\n",
            "features.1.0.block.1.fc2.bias: torch.Size([40])\n",
            "features.1.0.block.2.0.weight: torch.Size([24, 40, 1, 1])\n",
            "features.1.0.block.2.1.weight: torch.Size([24])\n",
            "features.1.0.block.2.1.bias: torch.Size([24])\n",
            "features.1.0.block.2.1.running_mean: torch.Size([24])\n",
            "features.1.0.block.2.1.running_var: torch.Size([24])\n",
            "features.1.0.block.2.1.num_batches_tracked: torch.Size([])\n",
            "features.1.1.block.0.0.weight: torch.Size([24, 1, 3, 3])\n",
            "features.1.1.block.0.1.weight: torch.Size([24])\n",
            "features.1.1.block.0.1.bias: torch.Size([24])\n",
            "features.1.1.block.0.1.running_mean: torch.Size([24])\n",
            "features.1.1.block.0.1.running_var: torch.Size([24])\n",
            "features.1.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.1.1.block.1.fc1.weight: torch.Size([6, 24, 1, 1])\n",
            "features.1.1.block.1.fc1.bias: torch.Size([6])\n",
            "features.1.1.block.1.fc2.weight: torch.Size([24, 6, 1, 1])\n",
            "features.1.1.block.1.fc2.bias: torch.Size([24])\n",
            "features.1.1.block.2.0.weight: torch.Size([24, 24, 1, 1])\n",
            "features.1.1.block.2.1.weight: torch.Size([24])\n",
            "features.1.1.block.2.1.bias: torch.Size([24])\n",
            "features.1.1.block.2.1.running_mean: torch.Size([24])\n",
            "features.1.1.block.2.1.running_var: torch.Size([24])\n",
            "features.1.1.block.2.1.num_batches_tracked: torch.Size([])\n",
            "features.2.0.block.0.0.weight: torch.Size([144, 24, 1, 1])\n",
            "features.2.0.block.0.1.weight: torch.Size([144])\n",
            "features.2.0.block.0.1.bias: torch.Size([144])\n",
            "features.2.0.block.0.1.running_mean: torch.Size([144])\n",
            "features.2.0.block.0.1.running_var: torch.Size([144])\n",
            "features.2.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.2.0.block.1.0.weight: torch.Size([144, 1, 3, 3])\n",
            "features.2.0.block.1.1.weight: torch.Size([144])\n",
            "features.2.0.block.1.1.bias: torch.Size([144])\n",
            "features.2.0.block.1.1.running_mean: torch.Size([144])\n",
            "features.2.0.block.1.1.running_var: torch.Size([144])\n",
            "features.2.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.2.0.block.2.fc1.weight: torch.Size([6, 144, 1, 1])\n",
            "features.2.0.block.2.fc1.bias: torch.Size([6])\n",
            "features.2.0.block.2.fc2.weight: torch.Size([144, 6, 1, 1])\n",
            "features.2.0.block.2.fc2.bias: torch.Size([144])\n",
            "features.2.0.block.3.0.weight: torch.Size([32, 144, 1, 1])\n",
            "features.2.0.block.3.1.weight: torch.Size([32])\n",
            "features.2.0.block.3.1.bias: torch.Size([32])\n",
            "features.2.0.block.3.1.running_mean: torch.Size([32])\n",
            "features.2.0.block.3.1.running_var: torch.Size([32])\n",
            "features.2.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.2.1.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
            "features.2.1.block.0.1.weight: torch.Size([192])\n",
            "features.2.1.block.0.1.bias: torch.Size([192])\n",
            "features.2.1.block.0.1.running_mean: torch.Size([192])\n",
            "features.2.1.block.0.1.running_var: torch.Size([192])\n",
            "features.2.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.2.1.block.1.0.weight: torch.Size([192, 1, 3, 3])\n",
            "features.2.1.block.1.1.weight: torch.Size([192])\n",
            "features.2.1.block.1.1.bias: torch.Size([192])\n",
            "features.2.1.block.1.1.running_mean: torch.Size([192])\n",
            "features.2.1.block.1.1.running_var: torch.Size([192])\n",
            "features.2.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.2.1.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
            "features.2.1.block.2.fc1.bias: torch.Size([8])\n",
            "features.2.1.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
            "features.2.1.block.2.fc2.bias: torch.Size([192])\n",
            "features.2.1.block.3.0.weight: torch.Size([32, 192, 1, 1])\n",
            "features.2.1.block.3.1.weight: torch.Size([32])\n",
            "features.2.1.block.3.1.bias: torch.Size([32])\n",
            "features.2.1.block.3.1.running_mean: torch.Size([32])\n",
            "features.2.1.block.3.1.running_var: torch.Size([32])\n",
            "features.2.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.2.2.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
            "features.2.2.block.0.1.weight: torch.Size([192])\n",
            "features.2.2.block.0.1.bias: torch.Size([192])\n",
            "features.2.2.block.0.1.running_mean: torch.Size([192])\n",
            "features.2.2.block.0.1.running_var: torch.Size([192])\n",
            "features.2.2.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.2.2.block.1.0.weight: torch.Size([192, 1, 3, 3])\n",
            "features.2.2.block.1.1.weight: torch.Size([192])\n",
            "features.2.2.block.1.1.bias: torch.Size([192])\n",
            "features.2.2.block.1.1.running_mean: torch.Size([192])\n",
            "features.2.2.block.1.1.running_var: torch.Size([192])\n",
            "features.2.2.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.2.2.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
            "features.2.2.block.2.fc1.bias: torch.Size([8])\n",
            "features.2.2.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
            "features.2.2.block.2.fc2.bias: torch.Size([192])\n",
            "features.2.2.block.3.0.weight: torch.Size([32, 192, 1, 1])\n",
            "features.2.2.block.3.1.weight: torch.Size([32])\n",
            "features.2.2.block.3.1.bias: torch.Size([32])\n",
            "features.2.2.block.3.1.running_mean: torch.Size([32])\n",
            "features.2.2.block.3.1.running_var: torch.Size([32])\n",
            "features.2.2.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.3.0.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
            "features.3.0.block.0.1.weight: torch.Size([192])\n",
            "features.3.0.block.0.1.bias: torch.Size([192])\n",
            "features.3.0.block.0.1.running_mean: torch.Size([192])\n",
            "features.3.0.block.0.1.running_var: torch.Size([192])\n",
            "features.3.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.3.0.block.1.0.weight: torch.Size([192, 1, 5, 5])\n",
            "features.3.0.block.1.1.weight: torch.Size([192])\n",
            "features.3.0.block.1.1.bias: torch.Size([192])\n",
            "features.3.0.block.1.1.running_mean: torch.Size([192])\n",
            "features.3.0.block.1.1.running_var: torch.Size([192])\n",
            "features.3.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.3.0.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
            "features.3.0.block.2.fc1.bias: torch.Size([8])\n",
            "features.3.0.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
            "features.3.0.block.2.fc2.bias: torch.Size([192])\n",
            "features.3.0.block.3.0.weight: torch.Size([48, 192, 1, 1])\n",
            "features.3.0.block.3.1.weight: torch.Size([48])\n",
            "features.3.0.block.3.1.bias: torch.Size([48])\n",
            "features.3.0.block.3.1.running_mean: torch.Size([48])\n",
            "features.3.0.block.3.1.running_var: torch.Size([48])\n",
            "features.3.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.3.1.block.0.0.weight: torch.Size([288, 48, 1, 1])\n",
            "features.3.1.block.0.1.weight: torch.Size([288])\n",
            "features.3.1.block.0.1.bias: torch.Size([288])\n",
            "features.3.1.block.0.1.running_mean: torch.Size([288])\n",
            "features.3.1.block.0.1.running_var: torch.Size([288])\n",
            "features.3.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.3.1.block.1.0.weight: torch.Size([288, 1, 5, 5])\n",
            "features.3.1.block.1.1.weight: torch.Size([288])\n",
            "features.3.1.block.1.1.bias: torch.Size([288])\n",
            "features.3.1.block.1.1.running_mean: torch.Size([288])\n",
            "features.3.1.block.1.1.running_var: torch.Size([288])\n",
            "features.3.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.3.1.block.2.fc1.weight: torch.Size([12, 288, 1, 1])\n",
            "features.3.1.block.2.fc1.bias: torch.Size([12])\n",
            "features.3.1.block.2.fc2.weight: torch.Size([288, 12, 1, 1])\n",
            "features.3.1.block.2.fc2.bias: torch.Size([288])\n",
            "features.3.1.block.3.0.weight: torch.Size([48, 288, 1, 1])\n",
            "features.3.1.block.3.1.weight: torch.Size([48])\n",
            "features.3.1.block.3.1.bias: torch.Size([48])\n",
            "features.3.1.block.3.1.running_mean: torch.Size([48])\n",
            "features.3.1.block.3.1.running_var: torch.Size([48])\n",
            "features.3.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.3.2.block.0.0.weight: torch.Size([288, 48, 1, 1])\n",
            "features.3.2.block.0.1.weight: torch.Size([288])\n",
            "features.3.2.block.0.1.bias: torch.Size([288])\n",
            "features.3.2.block.0.1.running_mean: torch.Size([288])\n",
            "features.3.2.block.0.1.running_var: torch.Size([288])\n",
            "features.3.2.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.3.2.block.1.0.weight: torch.Size([288, 1, 5, 5])\n",
            "features.3.2.block.1.1.weight: torch.Size([288])\n",
            "features.3.2.block.1.1.bias: torch.Size([288])\n",
            "features.3.2.block.1.1.running_mean: torch.Size([288])\n",
            "features.3.2.block.1.1.running_var: torch.Size([288])\n",
            "features.3.2.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.3.2.block.2.fc1.weight: torch.Size([12, 288, 1, 1])\n",
            "features.3.2.block.2.fc1.bias: torch.Size([12])\n",
            "features.3.2.block.2.fc2.weight: torch.Size([288, 12, 1, 1])\n",
            "features.3.2.block.2.fc2.bias: torch.Size([288])\n",
            "features.3.2.block.3.0.weight: torch.Size([48, 288, 1, 1])\n",
            "features.3.2.block.3.1.weight: torch.Size([48])\n",
            "features.3.2.block.3.1.bias: torch.Size([48])\n",
            "features.3.2.block.3.1.running_mean: torch.Size([48])\n",
            "features.3.2.block.3.1.running_var: torch.Size([48])\n",
            "features.3.2.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.4.0.block.0.0.weight: torch.Size([288, 48, 1, 1])\n",
            "features.4.0.block.0.1.weight: torch.Size([288])\n",
            "features.4.0.block.0.1.bias: torch.Size([288])\n",
            "features.4.0.block.0.1.running_mean: torch.Size([288])\n",
            "features.4.0.block.0.1.running_var: torch.Size([288])\n",
            "features.4.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.4.0.block.1.0.weight: torch.Size([288, 1, 3, 3])\n",
            "features.4.0.block.1.1.weight: torch.Size([288])\n",
            "features.4.0.block.1.1.bias: torch.Size([288])\n",
            "features.4.0.block.1.1.running_mean: torch.Size([288])\n",
            "features.4.0.block.1.1.running_var: torch.Size([288])\n",
            "features.4.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.4.0.block.2.fc1.weight: torch.Size([12, 288, 1, 1])\n",
            "features.4.0.block.2.fc1.bias: torch.Size([12])\n",
            "features.4.0.block.2.fc2.weight: torch.Size([288, 12, 1, 1])\n",
            "features.4.0.block.2.fc2.bias: torch.Size([288])\n",
            "features.4.0.block.3.0.weight: torch.Size([96, 288, 1, 1])\n",
            "features.4.0.block.3.1.weight: torch.Size([96])\n",
            "features.4.0.block.3.1.bias: torch.Size([96])\n",
            "features.4.0.block.3.1.running_mean: torch.Size([96])\n",
            "features.4.0.block.3.1.running_var: torch.Size([96])\n",
            "features.4.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.4.1.block.0.0.weight: torch.Size([576, 96, 1, 1])\n",
            "features.4.1.block.0.1.weight: torch.Size([576])\n",
            "features.4.1.block.0.1.bias: torch.Size([576])\n",
            "features.4.1.block.0.1.running_mean: torch.Size([576])\n",
            "features.4.1.block.0.1.running_var: torch.Size([576])\n",
            "features.4.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.4.1.block.1.0.weight: torch.Size([576, 1, 3, 3])\n",
            "features.4.1.block.1.1.weight: torch.Size([576])\n",
            "features.4.1.block.1.1.bias: torch.Size([576])\n",
            "features.4.1.block.1.1.running_mean: torch.Size([576])\n",
            "features.4.1.block.1.1.running_var: torch.Size([576])\n",
            "features.4.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.4.1.block.2.fc1.weight: torch.Size([24, 576, 1, 1])\n",
            "features.4.1.block.2.fc1.bias: torch.Size([24])\n",
            "features.4.1.block.2.fc2.weight: torch.Size([576, 24, 1, 1])\n",
            "features.4.1.block.2.fc2.bias: torch.Size([576])\n",
            "features.4.1.block.3.0.weight: torch.Size([96, 576, 1, 1])\n",
            "features.4.1.block.3.1.weight: torch.Size([96])\n",
            "features.4.1.block.3.1.bias: torch.Size([96])\n",
            "features.4.1.block.3.1.running_mean: torch.Size([96])\n",
            "features.4.1.block.3.1.running_var: torch.Size([96])\n",
            "features.4.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.4.2.block.0.0.weight: torch.Size([576, 96, 1, 1])\n",
            "features.4.2.block.0.1.weight: torch.Size([576])\n",
            "features.4.2.block.0.1.bias: torch.Size([576])\n",
            "features.4.2.block.0.1.running_mean: torch.Size([576])\n",
            "features.4.2.block.0.1.running_var: torch.Size([576])\n",
            "features.4.2.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.4.2.block.1.0.weight: torch.Size([576, 1, 3, 3])\n",
            "features.4.2.block.1.1.weight: torch.Size([576])\n",
            "features.4.2.block.1.1.bias: torch.Size([576])\n",
            "features.4.2.block.1.1.running_mean: torch.Size([576])\n",
            "features.4.2.block.1.1.running_var: torch.Size([576])\n",
            "features.4.2.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.4.2.block.2.fc1.weight: torch.Size([24, 576, 1, 1])\n",
            "features.4.2.block.2.fc1.bias: torch.Size([24])\n",
            "features.4.2.block.2.fc2.weight: torch.Size([576, 24, 1, 1])\n",
            "features.4.2.block.2.fc2.bias: torch.Size([576])\n",
            "features.4.2.block.3.0.weight: torch.Size([96, 576, 1, 1])\n",
            "features.4.2.block.3.1.weight: torch.Size([96])\n",
            "features.4.2.block.3.1.bias: torch.Size([96])\n",
            "features.4.2.block.3.1.running_mean: torch.Size([96])\n",
            "features.4.2.block.3.1.running_var: torch.Size([96])\n",
            "features.4.2.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.4.3.block.0.0.weight: torch.Size([576, 96, 1, 1])\n",
            "features.4.3.block.0.1.weight: torch.Size([576])\n",
            "features.4.3.block.0.1.bias: torch.Size([576])\n",
            "features.4.3.block.0.1.running_mean: torch.Size([576])\n",
            "features.4.3.block.0.1.running_var: torch.Size([576])\n",
            "features.4.3.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.4.3.block.1.0.weight: torch.Size([576, 1, 3, 3])\n",
            "features.4.3.block.1.1.weight: torch.Size([576])\n",
            "features.4.3.block.1.1.bias: torch.Size([576])\n",
            "features.4.3.block.1.1.running_mean: torch.Size([576])\n",
            "features.4.3.block.1.1.running_var: torch.Size([576])\n",
            "features.4.3.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.4.3.block.2.fc1.weight: torch.Size([24, 576, 1, 1])\n",
            "features.4.3.block.2.fc1.bias: torch.Size([24])\n",
            "features.4.3.block.2.fc2.weight: torch.Size([576, 24, 1, 1])\n",
            "features.4.3.block.2.fc2.bias: torch.Size([576])\n",
            "features.4.3.block.3.0.weight: torch.Size([96, 576, 1, 1])\n",
            "features.4.3.block.3.1.weight: torch.Size([96])\n",
            "features.4.3.block.3.1.bias: torch.Size([96])\n",
            "features.4.3.block.3.1.running_mean: torch.Size([96])\n",
            "features.4.3.block.3.1.running_var: torch.Size([96])\n",
            "features.4.3.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.4.4.block.0.0.weight: torch.Size([576, 96, 1, 1])\n",
            "features.4.4.block.0.1.weight: torch.Size([576])\n",
            "features.4.4.block.0.1.bias: torch.Size([576])\n",
            "features.4.4.block.0.1.running_mean: torch.Size([576])\n",
            "features.4.4.block.0.1.running_var: torch.Size([576])\n",
            "features.4.4.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.4.4.block.1.0.weight: torch.Size([576, 1, 3, 3])\n",
            "features.4.4.block.1.1.weight: torch.Size([576])\n",
            "features.4.4.block.1.1.bias: torch.Size([576])\n",
            "features.4.4.block.1.1.running_mean: torch.Size([576])\n",
            "features.4.4.block.1.1.running_var: torch.Size([576])\n",
            "features.4.4.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.4.4.block.2.fc1.weight: torch.Size([24, 576, 1, 1])\n",
            "features.4.4.block.2.fc1.bias: torch.Size([24])\n",
            "features.4.4.block.2.fc2.weight: torch.Size([576, 24, 1, 1])\n",
            "features.4.4.block.2.fc2.bias: torch.Size([576])\n",
            "features.4.4.block.3.0.weight: torch.Size([96, 576, 1, 1])\n",
            "features.4.4.block.3.1.weight: torch.Size([96])\n",
            "features.4.4.block.3.1.bias: torch.Size([96])\n",
            "features.4.4.block.3.1.running_mean: torch.Size([96])\n",
            "features.4.4.block.3.1.running_var: torch.Size([96])\n",
            "features.4.4.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.5.0.block.0.0.weight: torch.Size([576, 96, 1, 1])\n",
            "features.5.0.block.0.1.weight: torch.Size([576])\n",
            "features.5.0.block.0.1.bias: torch.Size([576])\n",
            "features.5.0.block.0.1.running_mean: torch.Size([576])\n",
            "features.5.0.block.0.1.running_var: torch.Size([576])\n",
            "features.5.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.5.0.block.1.0.weight: torch.Size([576, 1, 5, 5])\n",
            "features.5.0.block.1.1.weight: torch.Size([576])\n",
            "features.5.0.block.1.1.bias: torch.Size([576])\n",
            "features.5.0.block.1.1.running_mean: torch.Size([576])\n",
            "features.5.0.block.1.1.running_var: torch.Size([576])\n",
            "features.5.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.5.0.block.2.fc1.weight: torch.Size([24, 576, 1, 1])\n",
            "features.5.0.block.2.fc1.bias: torch.Size([24])\n",
            "features.5.0.block.2.fc2.weight: torch.Size([576, 24, 1, 1])\n",
            "features.5.0.block.2.fc2.bias: torch.Size([576])\n",
            "features.5.0.block.3.0.weight: torch.Size([136, 576, 1, 1])\n",
            "features.5.0.block.3.1.weight: torch.Size([136])\n",
            "features.5.0.block.3.1.bias: torch.Size([136])\n",
            "features.5.0.block.3.1.running_mean: torch.Size([136])\n",
            "features.5.0.block.3.1.running_var: torch.Size([136])\n",
            "features.5.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.5.1.block.0.0.weight: torch.Size([816, 136, 1, 1])\n",
            "features.5.1.block.0.1.weight: torch.Size([816])\n",
            "features.5.1.block.0.1.bias: torch.Size([816])\n",
            "features.5.1.block.0.1.running_mean: torch.Size([816])\n",
            "features.5.1.block.0.1.running_var: torch.Size([816])\n",
            "features.5.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.5.1.block.1.0.weight: torch.Size([816, 1, 5, 5])\n",
            "features.5.1.block.1.1.weight: torch.Size([816])\n",
            "features.5.1.block.1.1.bias: torch.Size([816])\n",
            "features.5.1.block.1.1.running_mean: torch.Size([816])\n",
            "features.5.1.block.1.1.running_var: torch.Size([816])\n",
            "features.5.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.5.1.block.2.fc1.weight: torch.Size([34, 816, 1, 1])\n",
            "features.5.1.block.2.fc1.bias: torch.Size([34])\n",
            "features.5.1.block.2.fc2.weight: torch.Size([816, 34, 1, 1])\n",
            "features.5.1.block.2.fc2.bias: torch.Size([816])\n",
            "features.5.1.block.3.0.weight: torch.Size([136, 816, 1, 1])\n",
            "features.5.1.block.3.1.weight: torch.Size([136])\n",
            "features.5.1.block.3.1.bias: torch.Size([136])\n",
            "features.5.1.block.3.1.running_mean: torch.Size([136])\n",
            "features.5.1.block.3.1.running_var: torch.Size([136])\n",
            "features.5.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.5.2.block.0.0.weight: torch.Size([816, 136, 1, 1])\n",
            "features.5.2.block.0.1.weight: torch.Size([816])\n",
            "features.5.2.block.0.1.bias: torch.Size([816])\n",
            "features.5.2.block.0.1.running_mean: torch.Size([816])\n",
            "features.5.2.block.0.1.running_var: torch.Size([816])\n",
            "features.5.2.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.5.2.block.1.0.weight: torch.Size([816, 1, 5, 5])\n",
            "features.5.2.block.1.1.weight: torch.Size([816])\n",
            "features.5.2.block.1.1.bias: torch.Size([816])\n",
            "features.5.2.block.1.1.running_mean: torch.Size([816])\n",
            "features.5.2.block.1.1.running_var: torch.Size([816])\n",
            "features.5.2.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.5.2.block.2.fc1.weight: torch.Size([34, 816, 1, 1])\n",
            "features.5.2.block.2.fc1.bias: torch.Size([34])\n",
            "features.5.2.block.2.fc2.weight: torch.Size([816, 34, 1, 1])\n",
            "features.5.2.block.2.fc2.bias: torch.Size([816])\n",
            "features.5.2.block.3.0.weight: torch.Size([136, 816, 1, 1])\n",
            "features.5.2.block.3.1.weight: torch.Size([136])\n",
            "features.5.2.block.3.1.bias: torch.Size([136])\n",
            "features.5.2.block.3.1.running_mean: torch.Size([136])\n",
            "features.5.2.block.3.1.running_var: torch.Size([136])\n",
            "features.5.2.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.5.3.block.0.0.weight: torch.Size([816, 136, 1, 1])\n",
            "features.5.3.block.0.1.weight: torch.Size([816])\n",
            "features.5.3.block.0.1.bias: torch.Size([816])\n",
            "features.5.3.block.0.1.running_mean: torch.Size([816])\n",
            "features.5.3.block.0.1.running_var: torch.Size([816])\n",
            "features.5.3.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.5.3.block.1.0.weight: torch.Size([816, 1, 5, 5])\n",
            "features.5.3.block.1.1.weight: torch.Size([816])\n",
            "features.5.3.block.1.1.bias: torch.Size([816])\n",
            "features.5.3.block.1.1.running_mean: torch.Size([816])\n",
            "features.5.3.block.1.1.running_var: torch.Size([816])\n",
            "features.5.3.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.5.3.block.2.fc1.weight: torch.Size([34, 816, 1, 1])\n",
            "features.5.3.block.2.fc1.bias: torch.Size([34])\n",
            "features.5.3.block.2.fc2.weight: torch.Size([816, 34, 1, 1])\n",
            "features.5.3.block.2.fc2.bias: torch.Size([816])\n",
            "features.5.3.block.3.0.weight: torch.Size([136, 816, 1, 1])\n",
            "features.5.3.block.3.1.weight: torch.Size([136])\n",
            "features.5.3.block.3.1.bias: torch.Size([136])\n",
            "features.5.3.block.3.1.running_mean: torch.Size([136])\n",
            "features.5.3.block.3.1.running_var: torch.Size([136])\n",
            "features.5.3.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.5.4.block.0.0.weight: torch.Size([816, 136, 1, 1])\n",
            "features.5.4.block.0.1.weight: torch.Size([816])\n",
            "features.5.4.block.0.1.bias: torch.Size([816])\n",
            "features.5.4.block.0.1.running_mean: torch.Size([816])\n",
            "features.5.4.block.0.1.running_var: torch.Size([816])\n",
            "features.5.4.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.5.4.block.1.0.weight: torch.Size([816, 1, 5, 5])\n",
            "features.5.4.block.1.1.weight: torch.Size([816])\n",
            "features.5.4.block.1.1.bias: torch.Size([816])\n",
            "features.5.4.block.1.1.running_mean: torch.Size([816])\n",
            "features.5.4.block.1.1.running_var: torch.Size([816])\n",
            "features.5.4.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.5.4.block.2.fc1.weight: torch.Size([34, 816, 1, 1])\n",
            "features.5.4.block.2.fc1.bias: torch.Size([34])\n",
            "features.5.4.block.2.fc2.weight: torch.Size([816, 34, 1, 1])\n",
            "features.5.4.block.2.fc2.bias: torch.Size([816])\n",
            "features.5.4.block.3.0.weight: torch.Size([136, 816, 1, 1])\n",
            "features.5.4.block.3.1.weight: torch.Size([136])\n",
            "features.5.4.block.3.1.bias: torch.Size([136])\n",
            "features.5.4.block.3.1.running_mean: torch.Size([136])\n",
            "features.5.4.block.3.1.running_var: torch.Size([136])\n",
            "features.5.4.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.0.block.0.0.weight: torch.Size([816, 136, 1, 1])\n",
            "features.6.0.block.0.1.weight: torch.Size([816])\n",
            "features.6.0.block.0.1.bias: torch.Size([816])\n",
            "features.6.0.block.0.1.running_mean: torch.Size([816])\n",
            "features.6.0.block.0.1.running_var: torch.Size([816])\n",
            "features.6.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.0.block.1.0.weight: torch.Size([816, 1, 5, 5])\n",
            "features.6.0.block.1.1.weight: torch.Size([816])\n",
            "features.6.0.block.1.1.bias: torch.Size([816])\n",
            "features.6.0.block.1.1.running_mean: torch.Size([816])\n",
            "features.6.0.block.1.1.running_var: torch.Size([816])\n",
            "features.6.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.0.block.2.fc1.weight: torch.Size([34, 816, 1, 1])\n",
            "features.6.0.block.2.fc1.bias: torch.Size([34])\n",
            "features.6.0.block.2.fc2.weight: torch.Size([816, 34, 1, 1])\n",
            "features.6.0.block.2.fc2.bias: torch.Size([816])\n",
            "features.6.0.block.3.0.weight: torch.Size([232, 816, 1, 1])\n",
            "features.6.0.block.3.1.weight: torch.Size([232])\n",
            "features.6.0.block.3.1.bias: torch.Size([232])\n",
            "features.6.0.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.0.block.3.1.running_var: torch.Size([232])\n",
            "features.6.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.1.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.6.1.block.0.1.weight: torch.Size([1392])\n",
            "features.6.1.block.0.1.bias: torch.Size([1392])\n",
            "features.6.1.block.0.1.running_mean: torch.Size([1392])\n",
            "features.6.1.block.0.1.running_var: torch.Size([1392])\n",
            "features.6.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.1.block.1.0.weight: torch.Size([1392, 1, 5, 5])\n",
            "features.6.1.block.1.1.weight: torch.Size([1392])\n",
            "features.6.1.block.1.1.bias: torch.Size([1392])\n",
            "features.6.1.block.1.1.running_mean: torch.Size([1392])\n",
            "features.6.1.block.1.1.running_var: torch.Size([1392])\n",
            "features.6.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.1.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.6.1.block.2.fc1.bias: torch.Size([58])\n",
            "features.6.1.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.6.1.block.2.fc2.bias: torch.Size([1392])\n",
            "features.6.1.block.3.0.weight: torch.Size([232, 1392, 1, 1])\n",
            "features.6.1.block.3.1.weight: torch.Size([232])\n",
            "features.6.1.block.3.1.bias: torch.Size([232])\n",
            "features.6.1.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.1.block.3.1.running_var: torch.Size([232])\n",
            "features.6.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.2.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.6.2.block.0.1.weight: torch.Size([1392])\n",
            "features.6.2.block.0.1.bias: torch.Size([1392])\n",
            "features.6.2.block.0.1.running_mean: torch.Size([1392])\n",
            "features.6.2.block.0.1.running_var: torch.Size([1392])\n",
            "features.6.2.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.2.block.1.0.weight: torch.Size([1392, 1, 5, 5])\n",
            "features.6.2.block.1.1.weight: torch.Size([1392])\n",
            "features.6.2.block.1.1.bias: torch.Size([1392])\n",
            "features.6.2.block.1.1.running_mean: torch.Size([1392])\n",
            "features.6.2.block.1.1.running_var: torch.Size([1392])\n",
            "features.6.2.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.2.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.6.2.block.2.fc1.bias: torch.Size([58])\n",
            "features.6.2.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.6.2.block.2.fc2.bias: torch.Size([1392])\n",
            "features.6.2.block.3.0.weight: torch.Size([232, 1392, 1, 1])\n",
            "features.6.2.block.3.1.weight: torch.Size([232])\n",
            "features.6.2.block.3.1.bias: torch.Size([232])\n",
            "features.6.2.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.2.block.3.1.running_var: torch.Size([232])\n",
            "features.6.2.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.3.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.6.3.block.0.1.weight: torch.Size([1392])\n",
            "features.6.3.block.0.1.bias: torch.Size([1392])\n",
            "features.6.3.block.0.1.running_mean: torch.Size([1392])\n",
            "features.6.3.block.0.1.running_var: torch.Size([1392])\n",
            "features.6.3.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.3.block.1.0.weight: torch.Size([1392, 1, 5, 5])\n",
            "features.6.3.block.1.1.weight: torch.Size([1392])\n",
            "features.6.3.block.1.1.bias: torch.Size([1392])\n",
            "features.6.3.block.1.1.running_mean: torch.Size([1392])\n",
            "features.6.3.block.1.1.running_var: torch.Size([1392])\n",
            "features.6.3.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.3.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.6.3.block.2.fc1.bias: torch.Size([58])\n",
            "features.6.3.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.6.3.block.2.fc2.bias: torch.Size([1392])\n",
            "features.6.3.block.3.0.weight: torch.Size([232, 1392, 1, 1])\n",
            "features.6.3.block.3.1.weight: torch.Size([232])\n",
            "features.6.3.block.3.1.bias: torch.Size([232])\n",
            "features.6.3.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.3.block.3.1.running_var: torch.Size([232])\n",
            "features.6.3.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.4.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.6.4.block.0.1.weight: torch.Size([1392])\n",
            "features.6.4.block.0.1.bias: torch.Size([1392])\n",
            "features.6.4.block.0.1.running_mean: torch.Size([1392])\n",
            "features.6.4.block.0.1.running_var: torch.Size([1392])\n",
            "features.6.4.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.4.block.1.0.weight: torch.Size([1392, 1, 5, 5])\n",
            "features.6.4.block.1.1.weight: torch.Size([1392])\n",
            "features.6.4.block.1.1.bias: torch.Size([1392])\n",
            "features.6.4.block.1.1.running_mean: torch.Size([1392])\n",
            "features.6.4.block.1.1.running_var: torch.Size([1392])\n",
            "features.6.4.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.4.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.6.4.block.2.fc1.bias: torch.Size([58])\n",
            "features.6.4.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.6.4.block.2.fc2.bias: torch.Size([1392])\n",
            "features.6.4.block.3.0.weight: torch.Size([232, 1392, 1, 1])\n",
            "features.6.4.block.3.1.weight: torch.Size([232])\n",
            "features.6.4.block.3.1.bias: torch.Size([232])\n",
            "features.6.4.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.4.block.3.1.running_var: torch.Size([232])\n",
            "features.6.4.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.6.5.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.6.5.block.0.1.weight: torch.Size([1392])\n",
            "features.6.5.block.0.1.bias: torch.Size([1392])\n",
            "features.6.5.block.0.1.running_mean: torch.Size([1392])\n",
            "features.6.5.block.0.1.running_var: torch.Size([1392])\n",
            "features.6.5.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.6.5.block.1.0.weight: torch.Size([1392, 1, 5, 5])\n",
            "features.6.5.block.1.1.weight: torch.Size([1392])\n",
            "features.6.5.block.1.1.bias: torch.Size([1392])\n",
            "features.6.5.block.1.1.running_mean: torch.Size([1392])\n",
            "features.6.5.block.1.1.running_var: torch.Size([1392])\n",
            "features.6.5.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.6.5.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.6.5.block.2.fc1.bias: torch.Size([58])\n",
            "features.6.5.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.6.5.block.2.fc2.bias: torch.Size([1392])\n",
            "features.6.5.block.3.0.weight: torch.Size([232, 1392, 1, 1])\n",
            "features.6.5.block.3.1.weight: torch.Size([232])\n",
            "features.6.5.block.3.1.bias: torch.Size([232])\n",
            "features.6.5.block.3.1.running_mean: torch.Size([232])\n",
            "features.6.5.block.3.1.running_var: torch.Size([232])\n",
            "features.6.5.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.7.0.block.0.0.weight: torch.Size([1392, 232, 1, 1])\n",
            "features.7.0.block.0.1.weight: torch.Size([1392])\n",
            "features.7.0.block.0.1.bias: torch.Size([1392])\n",
            "features.7.0.block.0.1.running_mean: torch.Size([1392])\n",
            "features.7.0.block.0.1.running_var: torch.Size([1392])\n",
            "features.7.0.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.7.0.block.1.0.weight: torch.Size([1392, 1, 3, 3])\n",
            "features.7.0.block.1.1.weight: torch.Size([1392])\n",
            "features.7.0.block.1.1.bias: torch.Size([1392])\n",
            "features.7.0.block.1.1.running_mean: torch.Size([1392])\n",
            "features.7.0.block.1.1.running_var: torch.Size([1392])\n",
            "features.7.0.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.7.0.block.2.fc1.weight: torch.Size([58, 1392, 1, 1])\n",
            "features.7.0.block.2.fc1.bias: torch.Size([58])\n",
            "features.7.0.block.2.fc2.weight: torch.Size([1392, 58, 1, 1])\n",
            "features.7.0.block.2.fc2.bias: torch.Size([1392])\n",
            "features.7.0.block.3.0.weight: torch.Size([384, 1392, 1, 1])\n",
            "features.7.0.block.3.1.weight: torch.Size([384])\n",
            "features.7.0.block.3.1.bias: torch.Size([384])\n",
            "features.7.0.block.3.1.running_mean: torch.Size([384])\n",
            "features.7.0.block.3.1.running_var: torch.Size([384])\n",
            "features.7.0.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.7.1.block.0.0.weight: torch.Size([2304, 384, 1, 1])\n",
            "features.7.1.block.0.1.weight: torch.Size([2304])\n",
            "features.7.1.block.0.1.bias: torch.Size([2304])\n",
            "features.7.1.block.0.1.running_mean: torch.Size([2304])\n",
            "features.7.1.block.0.1.running_var: torch.Size([2304])\n",
            "features.7.1.block.0.1.num_batches_tracked: torch.Size([])\n",
            "features.7.1.block.1.0.weight: torch.Size([2304, 1, 3, 3])\n",
            "features.7.1.block.1.1.weight: torch.Size([2304])\n",
            "features.7.1.block.1.1.bias: torch.Size([2304])\n",
            "features.7.1.block.1.1.running_mean: torch.Size([2304])\n",
            "features.7.1.block.1.1.running_var: torch.Size([2304])\n",
            "features.7.1.block.1.1.num_batches_tracked: torch.Size([])\n",
            "features.7.1.block.2.fc1.weight: torch.Size([96, 2304, 1, 1])\n",
            "features.7.1.block.2.fc1.bias: torch.Size([96])\n",
            "features.7.1.block.2.fc2.weight: torch.Size([2304, 96, 1, 1])\n",
            "features.7.1.block.2.fc2.bias: torch.Size([2304])\n",
            "features.7.1.block.3.0.weight: torch.Size([384, 2304, 1, 1])\n",
            "features.7.1.block.3.1.weight: torch.Size([384])\n",
            "features.7.1.block.3.1.bias: torch.Size([384])\n",
            "features.7.1.block.3.1.running_mean: torch.Size([384])\n",
            "features.7.1.block.3.1.running_var: torch.Size([384])\n",
            "features.7.1.block.3.1.num_batches_tracked: torch.Size([])\n",
            "features.8.0.weight: torch.Size([1536, 384, 1, 1])\n",
            "features.8.1.weight: torch.Size([1536])\n",
            "features.8.1.bias: torch.Size([1536])\n",
            "features.8.1.running_mean: torch.Size([1536])\n",
            "features.8.1.running_var: torch.Size([1536])\n",
            "features.8.1.num_batches_tracked: torch.Size([])\n",
            "classifier.1.weight: torch.Size([512, 1536])\n",
            "classifier.1.bias: torch.Size([512])\n",
            "classifier.4.weight: torch.Size([38, 512])\n",
            "classifier.4.bias: torch.Size([38])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_features, 2)  # 2 classes: Diseased, Healthy\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match EfficientNet input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "image_path = input(\"Enter image path: \")\n",
        "image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n",
        "image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    predicted_class = torch.argmax(output, dim=1).item()\n",
        "\n",
        "class_names = [\"Healthy\", \"Diseased\"]\n",
        "print(f\"Prediction: {class_names[predicted_class]}\")"
      ],
      "metadata": {
        "id": "vWlOyhXVEAcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "243ea367-9b40-4579-9afa-8c32428902e5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter image path: /content/disease.jpg\n",
            "Prediction: Healthy\n"
          ]
        }
      ]
    }
  ]
}